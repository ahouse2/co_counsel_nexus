---
services:
  # Include all existing services from infra/docker-compose.yml
  api:
    build:
      context: ./backend
      dockerfile: Dockerfile
    environment:
      MODEL_PROVIDERS_PRIMARY: ${MODEL_PROVIDERS_PRIMARY:-gemini}
      MODEL_PROVIDERS_SECONDARY: ${MODEL_PROVIDERS_SECONDARY:-openai}
      DEFAULT_CHAT_MODEL: ${DEFAULT_CHAT_MODEL:-gemini-2.5-flash}
      DEFAULT_EMBEDDING_MODEL: ${DEFAULT_EMBEDDING_MODEL:-text-embedding-004}
      DEFAULT_VISION_MODEL: ${DEFAULT_VISION_MODEL:-gemini-2.5-flash}
      NEO4J_URI: neo4j://neo4j:7687
      NEO4J_USER: neo4j
      NEO4J_PASSWORD: password
      QDRANT_URL: http://qdrant:6333
      VECTOR_DIR: /data/vector
      VOICE_SESSIONS_DIR: /data/voice/sessions
      VOICE_CACHE_DIR: /models
      DOCUMENT_STORAGE_PATH: /var/cocounsel/documents
      GRAPH_SNAPSHOT_PATH: /var/cocounsel/graphs
      TELEMETRY_BUFFER_PATH: /var/cocounsel/telemetry
      BILLING_USAGE_PATH: /data/billing/usage.json
      BILLING_DEFAULT_PLAN: ${BILLING_DEFAULT_PLAN:-community}
      TELEMETRY_ENABLED: ${TELEMETRY_ENABLED:-false}
      TELEMETRY_OTLP_ENDPOINT: ${TELEMETRY_OTLP_ENDPOINT:-http://otel-collector:4317}
      TELEMETRY_OTLP_INSECURE: ${TELEMETRY_OTLP_INSECURE:-true}
      TELEMETRY_ENVIRONMENT: ${TELEMETRY_ENVIRONMENT:-community}
      STT_SERVICE_URL: ${STT_SERVICE_URL:-http://stt:9000}
      TTS_SERVICE_URL: ${TTS_SERVICE_URL:-http://tts:5002}
      HUGGINGFACE_HUB_CACHE: /var/cocounsel/models/huggingface
      WHISPER_MODEL_PATH: /var/cocounsel/models/whisper
      TTS_MODEL_PATH: /var/cocounsel/models/tts
      SECRET_KEY: ${SECRET_KEY}
      # Ingestion Configuration
      INGESTION_VISION_MODEL: gemini-2.0-flash-exp
      GEMINI_API_KEY: ${GEMINI_API_KEY}
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      AZURE_OPENAI_API_KEY: ${AZURE_OPENAI_API_KEY}
      AZURE_OPENAI_ENDPOINT: ${AZURE_OPENAI_ENDPOINT}
      AZURE_OPENAI_API_VERSION: ${AZURE_OPENAI_API_VERSION}
      AZURE_OPENAI_CHAT_DEPLOYMENT: ${AZURE_OPENAI_CHAT_DEPLOYMENT}
      AZURE_OPENAI_EMBEDDING_DEPLOYMENT: ${AZURE_OPENAI_EMBEDDING_DEPLOYMENT}
      # Ingestion-specific Azure OpenAI Configuration
      INGESTION_AZURE_OPENAI_ENDPOINT: ${INGESTION_AZURE_OPENAI_ENDPOINT:-${AZURE_OPENAI_ENDPOINT}}
      INGESTION_AZURE_OPENAI_CHAT_DEPLOYMENT: ${INGESTION_AZURE_OPENAI_CHAT_DEPLOYMENT:-${AZURE_OPENAI_CHAT_DEPLOYMENT}}
      INGESTION_AZURE_OPENAI_EMBEDDING_DEPLOYMENT: ${INGESTION_AZURE_OPENAI_EMBEDDING_DEPLOYMENT:-${AZURE_OPENAI_EMBEDDING_DEPLOYMENT}}
      INGESTION_AZURE_OPENAI_API_VERSION: ${INGESTION_AZURE_OPENAI_API_VERSION:-${AZURE_OPENAI_API_VERSION}}
      INGESTION_ENTERPRISE_LLM_API_KEY: ${INGESTION_ENTERPRISE_LLM_API_KEY:-${AZURE_OPENAI_API_KEY}}
      INGESTION_ENTERPRISE_EMBEDDING_API_KEY: ${INGESTION_ENTERPRISE_EMBEDDING_API_KEY:-${AZURE_OPENAI_API_KEY}}
      COURTLISTENER_API_KEY: ${COURTLISTENER_API_KEY}
      INGESTION_COST_MODE: ${INGESTION_COST_MODE:-community}

    ports:
      - "8001:8000"
    depends_on:
      - neo4j
      - qdrant
      - postgres
    networks:
      - backend
    volumes:
      # - ./backend:/src/backend
      # - ./data:/data

      - voice_models:/models
      - storage_documents:/var/cocounsel/documents
      - storage_graphs:/var/cocounsel/graphs
      - storage_telemetry:/var/cocounsel/telemetry
      - models_huggingface:/var/cocounsel/models/huggingface
      - models_whisper:/var/cocounsel/models/whisper
      - models_tts:/var/cocounsel/models/tts
      # - ./backend:/src/backend # Disabled due to volume mount issues
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8000/health" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 12G
        reservations:
          cpus: '0.5'
          memory: 2G

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    ports:
      - "0.0.0.0:8090:80"
    networks:
      - backend
    depends_on:
      - api
    healthcheck:
      test: [ "CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://127.0.0.1:80" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 128M

  neo4j:
    image: neo4j:5.23.0
    container_name: neo4j
    ports:
      - "7474:7474"
      - "7687:7687"
    environment:
      NEO4J_AUTH: neo4j/${NEO4J_PASSWORD:-change_this_strong_password_123!}
    volumes:
      - neo4j_data:/data
    networks:
      - backend

  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_data:/qdrant/storage
    networks:
      - backend

  postgres:
    image: postgres:16-alpine
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-cocounsel}
      POSTGRES_USER: ${POSTGRES_USER:-cocounsel}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-change_this_strong_pg_password_456!}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - backend

  stt:
    image: fedirz/faster-whisper-server:latest-cuda
    runtime: runc
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: ${GPU_DEVICE_COUNT:-0}
              capabilities: [ gpu ]
    environment:
      ASR_MODEL: ${STT_MODEL_NAME:-openai/whisper-small}
      ASR_ENGINE: faster_whisper
      ASR_BEAM_SIZE: 5
      ASR_DEVICE: ${STT_DEVICE:-cpu}
      HUGGINGFACE_HUB_CACHE: /models/huggingface
      ASR_OUTPUT_LANGUAGE: ${STT_OUTPUT_LANGUAGE:-en}
    ports:
      - "9000:9000"
    volumes:
      - models_huggingface:/models/huggingface
      - models_whisper:/models/whisper
    networks:
      - backend

  tts:
    image: rhasspy/larynx:latest
    runtime: runc
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: ${GPU_DEVICE_COUNT:-0}
              capabilities: [ gpu ]
    environment:
      LARYNX_VOICE: ${TTS_VOICE:-en-us-blizzard_lessac}
      LARYNX_OUTPUT_DIR: /output
      HUGGINGFACE_HUB_CACHE: /models/huggingface
    ports:
      - "5002:5002"
    volumes:
      - models_huggingface:/models/huggingface
      - models_tts:/models/tts
      # - ./var/audio:/output  # Commented out - causing mount path error
    networks:
      - backend

  otel-collector:
    build:
      context: ./infra
      dockerfile: Dockerfile.otel
    ports:
      - "4317:4317"
      - "9464:9464"
    networks:
      - backend
  # Temporarily disabled due to Windows Docker Desktop mount path issues
  # storage-backup:
  #   image: ghcr.io/offen/docker-volume-backup:latest
  #   environment:
  #     BACKUP_CRON: ${BACKUP_CRON_SCHEDULE:-0 3 * * *}
  #     BACKUP_FILENAME: full-stack
  #     BACKUP_PATH: /var/backups
  #     BACKUP_PRUNING_PREFIX: full-stack
  #     BACKUP_PRUNING_KEEP_DAYS: ${BACKUP_RETENTION_DAYS:-7}
  #     BACKUP_LATEST_SYMLINK: true
  #   volumes:
  #     - ./var/backups:/var/backups
  #     - ./var/storage/documents:/backup/documents:ro
  #     - ./var/storage/graphs:/backup/graphs:ro
  #     - ./var/storage/telemetry:/backup/telemetry:ro
  #   networks:
  #     - backend

volumes:
  neo4j_data:
  qdrant_data:

  voice_models:
  postgres_data:
  storage_documents:
  storage_graphs:
  storage_telemetry:
  models_huggingface:
  models_whisper:
  models_tts:


networks:
  backend:
    driver: bridge
