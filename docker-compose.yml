---
services:
  # Include all existing services from infra/docker-compose.yml
  api:
    build:
      context: ./backend
      dockerfile: Dockerfile
    environment:
      MODEL_PROVIDERS_PRIMARY: ${MODEL_PROVIDERS_PRIMARY:-gemini}
      MODEL_PROVIDERS_SECONDARY: ${MODEL_PROVIDERS_SECONDARY:-openai}
      DEFAULT_CHAT_MODEL: ${DEFAULT_CHAT_MODEL:-gemini-2.5-flash}
      DEFAULT_EMBEDDING_MODEL: ${DEFAULT_EMBEDDING_MODEL:-text-embedding-004}
      DEFAULT_VISION_MODEL: ${DEFAULT_VISION_MODEL:-gemini-2.5-flash}
      NEO4J_URI: neo4j://neo4j:7687
      NEO4J_USER: neo4j
      NEO4J_PASSWORD: password
      QDRANT_URL: http://qdrant:6333
      VECTOR_DIR: /data/vector
      VOICE_SESSIONS_DIR: /data/voice/sessions
      VOICE_CACHE_DIR: /models
      DOCUMENT_STORAGE_PATH: /var/cocounsel/documents
      GRAPH_SNAPSHOT_PATH: /var/cocounsel/graphs
      TELEMETRY_BUFFER_PATH: /var/cocounsel/telemetry
      BILLING_USAGE_PATH: /data/billing/usage.json
      BILLING_DEFAULT_PLAN: ${BILLING_DEFAULT_PLAN:-community}
      TELEMETRY_ENABLED: ${TELEMETRY_ENABLED:-false}
      TELEMETRY_OTLP_ENDPOINT: ${TELEMETRY_OTLP_ENDPOINT:-http://otel-collector:4317}
      TELEMETRY_OTLP_INSECURE: ${TELEMETRY_OTLP_INSECURE:-true}
      TELEMETRY_ENVIRONMENT: ${TELEMETRY_ENVIRONMENT:-community}
      STT_SERVICE_URL: ${STT_SERVICE_URL:-http://stt:9000}
      TTS_SERVICE_URL: ${TTS_SERVICE_URL:-http://tts:5002}
      HUGGINGFACE_HUB_CACHE: /var/cocounsel/models/huggingface
      WHISPER_MODEL_PATH: /var/cocounsel/models/whisper
      TTS_MODEL_PATH: /var/cocounsel/models/tts
      SECRET_KEY: ${SECRET_KEY}
      # Ingestion Configuration
      INGESTION_VISION_MODEL: gemini-2.0-flash-exp
      INGESTION_VISION_API_KEY: ${GEMINI_API_KEY}
    ports:
      - "8000:8000"
    depends_on:
      - neo4j
      - qdrant
      - postgres
    networks:
      - backend
    volumes:
      - api_data:/data
      - voice_models:/models
      - ./var/storage/documents:/var/cocounsel/documents
      - ./var/storage/graphs:/var/cocounsel/graphs
      - ./var/storage/telemetry:/var/cocounsel/telemetry
      - ./var/models/huggingface:/var/cocounsel/models/huggingface
      - ./var/models/whisper:/var/cocounsel/models/whisper
      - ./var/models/tts:/var/cocounsel/models/tts
      # - ./backend:/src/backend  # Disabled due to volume mount issues

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    ports:
      - "8088:80"
    networks:
      - backend
    depends_on:
      - api

  neo4j:
    image: neo4j:5.23.0
    container_name: neo4j
    ports:
      - "7474:7474"
      - "7687:7687"
    environment:
      NEO4J_AUTH: neo4j/password
    volumes:
      - neo4j_data:/data
    networks:
      - backend

  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_data:/qdrant/storage
    networks:
      - backend

  postgres:
    image: postgres:16-alpine
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-cocounsel}
      POSTGRES_USER: ${POSTGRES_USER:-cocounsel}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-securepassword}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - backend

  stt:
    image: fedirz/faster-whisper-server:latest-cuda
    runtime: runc
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: ${GPU_DEVICE_COUNT:-0}
              capabilities: [ gpu ]
    environment:
      ASR_MODEL: ${STT_MODEL_NAME:-openai/whisper-small}
      ASR_ENGINE: faster_whisper
      ASR_BEAM_SIZE: 5
      ASR_DEVICE: ${STT_DEVICE:-cpu}
      HUGGINGFACE_HUB_CACHE: /models/huggingface
      ASR_OUTPUT_LANGUAGE: ${STT_OUTPUT_LANGUAGE:-en}
    ports:
      - "9000:9000"
    volumes:
      - ./var/models/huggingface:/models/huggingface
      - ./var/models/whisper:/models/whisper
    networks:
      - backend

  tts:
    image: rhasspy/larynx:latest
    runtime: runc
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: ${GPU_DEVICE_COUNT:-0}
              capabilities: [ gpu ]
    environment:
      LARYNX_VOICE: ${TTS_VOICE:-en-us-blizzard_lessac}
      LARYNX_OUTPUT_DIR: /output
      HUGGINGFACE_HUB_CACHE: /models/huggingface
    ports:
      - "5002:5002"
    volumes:
      - ./var/models/huggingface:/models/huggingface
      - ./var/models/tts:/models/tts
      - ./var/audio:/output
    networks:
      - backend

  otel-collector:
    build:
      context: ./infra
      dockerfile: Dockerfile.otel
    ports:
      - "4317:4317"
      - "9464:9464"
    networks:
      - backend

  storage-backup:
    image: ghcr.io/offen/docker-volume-backup:latest
    environment:
      BACKUP_CRON: ${BACKUP_CRON_SCHEDULE:-0 3 * * *}
      BACKUP_FILENAME: full-stack
      BACKUP_PATH: /var/backups
      BACKUP_PRUNING_PREFIX: full-stack
      BACKUP_PRUNING_KEEP_DAYS: ${BACKUP_RETENTION_DAYS:-7}
      BACKUP_LATEST_SYMLINK: true
    volumes:
      - ./var/backups:/var/backups
      - ./var/storage/documents:/backup/documents:ro
      - ./var/storage/graphs:/backup/graphs:ro
      - ./var/storage/telemetry:/backup/telemetry:ro
    networks:
      - backend

volumes:
  neo4j_data:
  qdrant_data:
  api_data:
  voice_models:
  postgres_data:


networks:
  backend:
    driver: bridge
